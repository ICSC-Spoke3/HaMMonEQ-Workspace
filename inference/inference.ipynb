{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00ff178",
   "metadata": {},
   "source": [
    "# Attention U-Net Inference Notebook\n",
    "\n",
    "This notebook demonstrates how to load and run inference using an **Attention U-Net** model.\n",
    "It is organized into three main sections:\n",
    "\n",
    "- **Model Loading & Initialization**: Load the trained model and set up the core components.\n",
    "\n",
    "- **Single Image Inference**: Perform inference on a single input image.\n",
    "\n",
    "- **Batch Inference**: Run inference on all images contained in a specified folder.\n",
    "\n",
    "Each section includes configurable parameters at the beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9348cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(Path(\"./HaMMon-ML-digital-twin\").resolve()))\n",
    "sys.path.append(str(Path(\"../local_src\").resolve()))\n",
    "\n",
    "from src.patcher import Patcher\n",
    "from src.predict import Predict\n",
    "from models.att_unet import sAttU_Net\n",
    "from datasets.public_cracks import Dataset\n",
    "from imgs_eq import ImgEQ as Img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f2c36",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Model Loading & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5 # Set your model threshold value\n",
    "debug_patches = False  # Set to True if you want to visualize patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1714cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an Img object using the data from Dataset\n",
    "img = Img(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and weights\n",
    "model = sAttU_Net(output_ch=1)\n",
    "map_location = torch.device('cpu') if not torch.cuda.is_available() else None\n",
    "if map_location:\n",
    "    print(\"No GPU available, loading model on CPU.\")\n",
    "weights = torch.load(\"./weights/007-sattunet-pc1-c50.pth\", map_location=map_location)\n",
    "\n",
    "model.load_state_dict(weights['data'])\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded and set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Predict object with the specified threshold\n",
    "predict = Predict(kind='binary_predictions', threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac815e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Patcher with the model, kernel size, stride, and device\n",
    "patcher = Patcher(\n",
    "    model=model, \n",
    "    kernel=(512, 512), \n",
    "    stride=(500, 500), \n",
    "    device=device, \n",
    "    mode='average', \n",
    "    predict=predict, \n",
    "    debug=debug_patches, \n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bbbca",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Single Image Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c64711",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'your_image_path_here.jpg'  # Replace with your image path\n",
    "output_dir = 'your_output_directory_here'  # Replace with your output directory\n",
    "alpha = 0.5  # Set the transparency level for superposition\n",
    "\n",
    "image_path = Path(image_path).resolve()\n",
    "if not image_path.exists():\n",
    "    raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "output_dir = Path(output_dir).resolve()\n",
    "if not output_dir.exists():\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory created: {output_dir}\")\n",
    "\n",
    "save_path = os.path.join(output_dir, image_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Path object for the image file\n",
    "file_path = Path(image_path)\n",
    "assert file_path.exists(), f\"File {file_path} does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and transform it \n",
    "input = Dataset.load_img(file_path)\n",
    "transform = Dataset.transform()\n",
    "input = transform(input)\n",
    "input = input.unsqueeze(0).to(device)\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the input image\n",
    "img_np = img.img_to_np(input.squeeze(0))\n",
    "img.view_np(img_np, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the input image\n",
    "with torch.no_grad():\n",
    "    output = patcher(input)\n",
    "    output = predict(output=output)\n",
    "    \n",
    "    \n",
    "type(output), output.shape, output.min(), output.max()\n",
    "print(\"Inference completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61881649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output to a numpy array for visualization\n",
    "prediction_np = img.label_to_np(output[0].squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be33848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction\n",
    "img.view_np(prediction_np, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba78ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the superposed image. This function overlays the input image and the prediction with transparency\n",
    "superposed_img = img.get_superposed_image(\n",
    "    img.img_to_np(input.squeeze(0)),\n",
    "    prediction_np,\n",
    "    alpha=alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c99605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction\n",
    "img.view_np(superposed_img, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547449cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the superposition with the input image and the prediction\n",
    "img.visualize_images_with_superposition_prediction(img.img_to_np(input.squeeze(0)), prediction_np, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the superposed image\n",
    "img.save_from_np(superposed_img, save_path=save_path)\n",
    "print(f\"Superposed image saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c23a2",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213db7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'your_image_directory_here'  # Replace with your image directory\n",
    "output_dir = 'your_output_directory_here'  # Replace with your output directory\n",
    "alpha = 0.5  # Set the transparency level for superposition\n",
    "\n",
    "image_dir = Path(image_dir).resolve()\n",
    "if not image_dir.exists():\n",
    "    raise FileNotFoundError(f\"Image not found: {image_dir}\")\n",
    "\n",
    "output_dir = Path(output_dir).resolve()\n",
    "if not output_dir.exists():\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory created: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in image_dir.glob(\"*.jpg\"):\n",
    "    print(f\"Processing file: {file}\")\n",
    "    save_path = os.path.join(output_dir, file.name)\n",
    "    \n",
    "    # Load the image and transform it \n",
    "    input = Dataset.load_img(file)\n",
    "    transform = Dataset.transform()\n",
    "    input = transform(input)\n",
    "    input = input.unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform inference on the input image\n",
    "    with torch.no_grad():\n",
    "        output = patcher(input)\n",
    "        output = predict(output=output)\n",
    "    \n",
    "    # Convert the output to a numpy array for visualization\n",
    "    prediction_np = img.label_to_np(output[0].squeeze(0))\n",
    "    \n",
    "    # Compute the superposed image\n",
    "    superposed_img = img.get_superposed_image(\n",
    "        img.img_to_np(input.squeeze(0)),\n",
    "        prediction_np,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    \n",
    "    # Save the superposed image\n",
    "    img.save_from_np(superposed_img, save_path=save_path)\n",
    "    print(f\"Superposed image saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
